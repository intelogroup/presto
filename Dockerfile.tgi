# Hugging Face Text Generation Inference (TGI) Dockerfile for Railway
# Alternative to vLLM for serving Llama 3.1

FROM ghcr.io/huggingface/text-generation-inference:3.3.5

# Set environment variables
ENV MODEL_ID="meta-llama/Meta-Llama-3.1-8B-Instruct"
ENV PORT=8000
ENV HOSTNAME=0.0.0.0

# TGI configuration
ENV MAX_CONCURRENT_REQUESTS=128
ENV MAX_BEST_OF=2
ENV MAX_STOP_SEQUENCES=4
ENV MAX_INPUT_LENGTH=4000
ENV MAX_TOTAL_TOKENS=4096
ENV WAITING_SERVED_RATIO=1.2
ENV MAX_BATCH_PREFILL_TOKENS=4096
ENV MAX_BATCH_TOTAL_TOKENS=8192

# Create startup script
RUN echo '#!/bin/bash\n\
echo "Starting TGI server with model: $MODEL_ID"\n\
echo "Server will be available at: http://$HOSTNAME:$PORT"\n\
\n\
# Start TGI server\n\
text-generation-launcher \\\n\
    --model-id "$MODEL_ID" \\\n\
    --hostname "$HOSTNAME" \\\n\
    --port "$PORT" \\\n\
    --max-concurrent-requests "$MAX_CONCURRENT_REQUESTS" \\\n\
    --max-best-of "$MAX_BEST_OF" \\\n\
    --max-stop-sequences "$MAX_STOP_SEQUENCES" \\\n\
    --max-input-length "$MAX_INPUT_LENGTH" \\\n\
    --max-total-tokens "$MAX_TOTAL_TOKENS" \\\n\
    --waiting-served-ratio "$WAITING_SERVED_RATIO" \\\n\
    --max-batch-prefill-tokens "$MAX_BATCH_PREFILL_TOKENS" \\\n\
    --max-batch-total-tokens "$MAX_BATCH_TOTAL_TOKENS" \\\n\
    --trust-remote-code' > /start-tgi.sh

RUN chmod +x /start-tgi.sh

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=300s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Start the server
CMD ["/start-tgi.sh"]