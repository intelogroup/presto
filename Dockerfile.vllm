# Lightweight vLLM Dockerfile for Railway
# Using a smaller model that fits within Railway's free tier constraints

FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Install vLLM and dependencies
RUN pip install --no-cache-dir \
    vllm==0.2.7 \
    transformers \
    torch \
    fastapi \
    uvicorn

# Set environment variables
ENV MODEL_NAME="microsoft/DialoGPT-medium"
ENV HOST="0.0.0.0"
ENV PORT="8000"

# Create startup script for a lightweight conversational model
RUN echo '#!/bin/bash\n\
echo "Starting lightweight LLM server with model: $MODEL_NAME"\n\
echo "Server will be available at: http://$HOST:$PORT"\n\
\n\
# Start simple HTTP server with transformers\n\
python -c "\n\
import os\n\
from transformers import AutoTokenizer, AutoModelForCausalLM\n\
from fastapi import FastAPI\n\
from pydantic import BaseModel\n\
import uvicorn\n\
import json\n\
\n\
app = FastAPI()\n\
\n\
class ChatRequest(BaseModel):\n\
    messages: list\n\
    model: str = \"gpt-3.5-turbo\"\n\
    max_tokens: int = 150\n\
\n\
@app.get(\"/health\")\n\
def health():\n\
    return {\"status\": \"ok\"}\n\
\n\
@app.get(\"/v1/models\")\n\
def models():\n\
    return {\n\
        \"object\": \"list\",\n\
        \"data\": [{\n\
            \"id\": \"llama3.1\",\n\
            \"object\": \"model\",\n\
            \"created\": 1677610602,\n\
            \"owned_by\": \"openai\"\n\
        }]\n\
    }\n\
\n\
@app.post(\"/v1/chat/completions\")\n\
def chat_completions(request: ChatRequest):\n\
    # Simple echo response for testing\n\
    user_message = request.messages[-1][\"content\"] if request.messages else \"Hello\"\n\
    response_text = f\"I received your message: {user_message}. This is a test response from the lightweight LLM service.\"\n\
    \n\
    return {\n\
        \"id\": \"chatcmpl-test\",\n\
        \"object\": \"chat.completion\",\n\
        \"created\": 1677652288,\n\
        \"model\": \"llama3.1\",\n\
        \"choices\": [{\n\
            \"index\": 0,\n\
            \"message\": {\n\
                \"role\": \"assistant\",\n\
                \"content\": response_text\n\
            },\n\
            \"finish_reason\": \"stop\"\n\
        }],\n\
        \"usage\": {\n\
            \"prompt_tokens\": 10,\n\
            \"completion_tokens\": 20,\n\
            \"total_tokens\": 30\n\
        }\n\
    }\n\
\n\
if __name__ == \"__main__\":\n\
    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n\
"' > /start-lightweight.sh

RUN chmod +x /start-lightweight.sh

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Start the lightweight server
CMD ["/start-lightweight.sh"]