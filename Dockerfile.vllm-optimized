# Optimized vLLM Dockerfile for Railway - Llama 3.1 Performance Tuned
# Based on vLLM best practices and Railway constraints

FROM nvidia/cuda:12.1-devel-ubuntu22.04

# Set environment variables for optimization
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV PYTHONOPTIMIZE=1
ENV CUDA_VISIBLE_DEVICES=0
ENV CUDA_LAUNCH_BLOCKING=0

# Install system dependencies with optimizations
RUN apt-get update && apt-get install -y \
    python3.11 \
    python3.11-dev \
    python3-pip \
    git \
    curl \
    wget \
    build-essential \
    ninja-build \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Create symbolic links for python
RUN ln -sf /usr/bin/python3.11 /usr/bin/python3 && \
    ln -sf /usr/bin/python3.11 /usr/bin/python

# Upgrade pip and install wheel
RUN python3 -m pip install --upgrade pip setuptools wheel

# Install PyTorch with CUDA support (optimized version)
RUN pip install --no-cache-dir \
    torch==2.1.2 \
    torchvision==0.16.2 \
    torchaudio==2.1.2 \
    --index-url https://download.pytorch.org/whl/cu121

# Install vLLM with optimizations
RUN pip install --no-cache-dir \
    vllm==0.3.3 \
    transformers==4.37.2 \
    accelerate==0.26.1 \
    tokenizers==0.15.1 \
    fastapi==0.108.0 \
    uvicorn[standard]==0.25.0 \
    pydantic==2.5.3 \
    numpy==1.24.4 \
    requests==2.31.0

# Install additional performance libraries
RUN pip install --no-cache-dir \
    flash-attn==2.5.0 \
    xformers==0.0.23.post1 \
    --no-build-isolation

# Set working directory
WORKDIR /app

# Create optimized directories
RUN mkdir -p /opt/models /opt/cache /app/logs

# Set environment variables for vLLM optimization
ENV MODEL_NAME="meta-llama/Meta-Llama-3.1-8B-Instruct"
ENV HOST="0.0.0.0"
ENV PORT="8000"
ENV VLLM_WORKER_USE_RAY="false"
ENV VLLM_ATTENTION_BACKEND="FLASHINFER"
ENV VLLM_TENSOR_PARALLEL_SIZE="1"
ENV VLLM_GPU_MEMORY_UTILIZATION="0.95"
ENV VLLM_MAX_NUM_BATCHED_TOKENS="16384"
ENV VLLM_MAX_NUM_SEQS="512"
ENV VLLM_NUM_SCHEDULER_STEPS="12"
ENV VLLM_MAX_SEQ_LEN_TO_CAPTURE="16384"
ENV VLLM_ENABLE_CHUNKED_PREFILL="false"
ENV VLLM_ENABLE_PREFIX_CACHING="false"
ENV VLLM_DISABLE_LOG_REQUESTS="true"
ENV VLLM_DISABLE_NUMA_BALANCING="true"
ENV NCCL_MIN_NCHANNELS="4"
ENV HF_HOME="/opt/cache"
ENV TRANSFORMERS_CACHE="/opt/cache"
ENV HF_DATASETS_CACHE="/opt/cache"

# Create optimized startup script
RUN echo '#!/bin/bash\n\
set -e\n\
\n\
echo "ðŸš€ Starting Optimized vLLM Server for Llama 3.1"\n\
echo "Model: $MODEL_NAME"\n\
echo "Host: $HOST:$PORT"\n\
echo "GPU Memory Utilization: $VLLM_GPU_MEMORY_UTILIZATION"\n\
echo "Max Batched Tokens: $VLLM_MAX_NUM_BATCHED_TOKENS"\n\
echo "Max Sequences: $VLLM_MAX_NUM_SEQS"\n\
echo "Scheduler Steps: $VLLM_NUM_SCHEDULER_STEPS"\n\
echo ""\n\
\n\
# System optimizations\n\
echo "Applying system optimizations..."\n\
if [ "$VLLM_DISABLE_NUMA_BALANCING" = "true" ]; then\n\
    echo 0 > /proc/sys/kernel/numa_balancing 2>/dev/null || echo "NUMA balancing optimization skipped (no permissions)"\n\
fi\n\
\n\
# GPU optimizations\n\
echo "Configuring GPU optimizations..."\n\
export CUDA_DEVICE_ORDER=PCI_BUS_ID\n\
export CUDA_VISIBLE_DEVICES=0\n\
\n\
# Memory optimizations\n\
echo "Configuring memory optimizations..."\n\
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n\
\n\
# Start vLLM with optimized parameters\n\
echo "Starting vLLM server with optimized configuration..."\n\
exec python -m vllm.entrypoints.openai.api_server \\\n\
    --model "$MODEL_NAME" \\\n\
    --host "$HOST" \\\n\
    --port "$PORT" \\\n\
    --tensor-parallel-size "$VLLM_TENSOR_PARALLEL_SIZE" \\\n\
    --gpu-memory-utilization "$VLLM_GPU_MEMORY_UTILIZATION" \\\n\
    --max-num-batched-tokens "$VLLM_MAX_NUM_BATCHED_TOKENS" \\\n\
    --max-num-seqs "$VLLM_MAX_NUM_SEQS" \\\n\
    --num-scheduler-steps "$VLLM_NUM_SCHEDULER_STEPS" \\\n\
    --max-seq-len-to-capture "$VLLM_MAX_SEQ_LEN_TO_CAPTURE" \\\n\
    --kv-cache-dtype auto \\\n\
    --served-model-name llama3.1 \\\n\
    --disable-log-requests \\\n\
    --trust-remote-code \\\n\
    --max-model-len 8192\n\
' > /start-vllm-optimized.sh

# Make startup script executable
RUN chmod +x /start-vllm-optimized.sh

# Create health check script
RUN echo '#!/bin/bash\n\
curl -f http://localhost:8000/health || exit 1\n\
' > /health-check.sh && chmod +x /health-check.sh

# Expose port
EXPOSE 8000

# Optimized health check
HEALTHCHECK --interval=30s --timeout=15s --start-period=180s --retries=5 \
    CMD /health-check.sh

# Set resource limits and optimizations
RUN echo 'net.core.somaxconn = 65535' >> /etc/sysctl.conf || true
RUN echo 'net.ipv4.tcp_max_syn_backlog = 65535' >> /etc/sysctl.conf || true

# Start the optimized vLLM server
CMD ["/start-vllm-optimized.sh"]