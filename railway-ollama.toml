# Railway configuration for Ollama LLM service
# This deploys Ollama with Llama 3.1 and Mistral models

[build]
# Use the Ollama-specific Dockerfile
dockerfilePath = "Dockerfile.ollama"

[deploy]
# Service configuration
startCommand = "/home/ollama/start-ollama.sh"
restartPolicyType = "ON_FAILURE"
restartPolicyMaxRetries = 3

# Health check configuration
healthcheckPath = "/api/tags"
healthcheckTimeout = 90

# Resource configuration
# Ollama needs more memory for model loading
[deploy.resources]
# Request higher memory for model storage
memory = "4Gi"
cpu = "2000m"

# Environment variables
[env]
OLLAMA_HOST = "0.0.0.0"
OLLAMA_PORT = "11434"
# Control which models to pull
PULL_LLAMA = "true"
# Ollama configuration
OLLAMA_NUM_PARALLEL = "1"
OLLAMA_MAX_LOADED_MODELS = "2"
OLLAMA_FLASH_ATTENTION = "1"

# Networking
[networking]
# Expose Ollama API port
ports = [11434]

# Volume for model storage (enable in Railway dashboard)
[volumes]
# Store models persistently across deployments
# Enable this volume in Railway dashboard and mount to /home/ollama/.ollama
models = "/home/ollama/.ollama"
