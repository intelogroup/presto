# Railway configuration for Hugging Face TGI service
# Alternative deployment option for Llama 3.1

[build]
# Use the TGI-specific Dockerfile
dockerfilePath = "Dockerfile.tgi"

[deploy]
# Service configuration
startCommand = "/start-tgi.sh"
restartPolicyType = "ON_FAILURE"
restartPolicyMaxRetries = 3

# Health check configuration
healthcheckPath = "/health"
healthcheckTimeout = 120

# Resource configuration
# TGI needs significant memory for model loading
[deploy.resources]
# Memory allocation for Llama 3.1 8B
memory = "8Gi"
cpu = "4000m"

# Environment variables
[env]
# Model configuration
MODEL_ID = "meta-llama/Meta-Llama-3.1-8B-Instruct"
HOSTNAME = "0.0.0.0"
PORT = "8000"

# Hugging Face token (set this in Railway dashboard)
# HF_TOKEN = "your-hf-token-here"

# TGI optimization settings
MAX_CONCURRENT_REQUESTS = "128"
MAX_BEST_OF = "2"
MAX_STOP_SEQUENCES = "4"
MAX_INPUT_LENGTH = "4000"
MAX_TOTAL_TOKENS = "4096"
WAITING_SERVED_RATIO = "1.2"
MAX_BATCH_PREFILL_TOKENS = "4096"
MAX_BATCH_TOTAL_TOKENS = "8192"

# Networking
[networking]
# Expose TGI API port
ports = [8000]

# Volume for model caching (optional)
[volumes]
# Cache models to avoid re-downloading
models = "/data"